#Tom's GUI code

from Tkinter import *
import tkMessageBox    #the * in previous line should already import this but need this line b/c of line 10 and 32
import requests, json, csv, os    #arcpy
from nytimesarticle import articleAPI
from geopy.geocoders import Nominatim
from HTMLParser import HTMLParser
from bs4 import BeautifulSoup
########################## TESTING USING: Clinton, 'New York', 20161108, 20161109

api = articleAPI('8a7bed87ddf84a98bd560ba2eb3ba3fb')  # NYT Article Search API key

# Function to saves the user's inputs into a text file
def save():
    text = E1.get() + "\n" + "glocations, " + E2.get() + "\n" + E3.get() + "\n" + E4.get()  # The .get() returns the user's query inputs that were typed in the entry text boxes as a string
    with open("QueryResults.txt", "w") as f:   # Creates and writes the user inputs into a text file which will be located in the same directory as this script
        f.writelines(text) # Writes the string set to the 'text' variable into the text file

# Function that creates the pop up message once the GO button is clicked on by the user
def saveMessage():
    tkMessageBox.showinfo("Query Message", "Success! Query in progress.")

# Function that executes the 'save' and then 'saveMessage' functions once the GO button is clicked on by the user
def buttonClicked():
    save()
    saveMessage()
    
      
top = Tk(className = "NYTimes Article Search GUI")    # Establishes the GUI and labels it with a title 
top.minsize(width = 350, height = 100)     # Sets the dimensions of the GUI

# Sets up the 5 labels seen on the left side of the GUI 
Label(top, text = "Search Words ***").grid(row=0)     # Searches through Headline, Body, and Byline
Label(top, text = "Geographic Location (Only one)").grid(row=1)    
Label(top, text = "Start Date (YYYYMMDD) ***").grid(row=2)
Label(top, text = "End Date (YYYYMMDD) ***").grid(row=3)

Label(top, text = "*** = Required Field").grid(row=4)    # Note to the user

# Sets up text boxes on the right side of the GUI for the user to type their query information into
E1 = Entry(top)
E2 = Entry(top)
E3 = Entry(top)
E4 = Entry(top)

E1.grid(row=0, column=1)   
E2.grid(row=1, column=1)
E3.grid(row=2, column=1)
E4.grid(row=3, column=1)

# Sets up the GO button at the botton of the GUI and specifies that when it is clicked, it call the buttonClicked function defined above
Button(top, text = "GO", command = buttonClicked).grid(row=5, column=0)

# Specifies the end of the GUI
top.mainloop()

# Path to where the above text file is located, again it is located in the same directory as this script
folderPath = "F:\\Tom_Smith_Python_Programming_Fall_2016\\Module_2_Comp_Programming_for_GIS\\Final_Project\\"
inputFN = "QueryResults.txt"
inputFilePath = folderPath + inputFN

# Reads in the text file and sets all the lines of the text file to the variable names 'fileString'
f1 = open(inputFilePath, "r")
fileString = f1.read()
f1.close()


queryList = fileString.split("\n")  # Creates a list in which each line of the text file is an element in the list 

wordSearch = queryList[0]  # User input keywords
print wordSearch  # Test

searchDateStart = queryList[2]  # User input begin date
print searchDateStart  # Test

searchDateEnd = queryList[3]  # User input end date
print searchDateEnd  # Test

geoLocation = queryList[1].split(", ")     # Separates the fq type of "glocations" and the user's location query input so that they can be indexed and later referenced individually 
HLURLnoLoc = {}  # Initialize empty dictionary to store headline:URL pairs when user specifies no location

if geoLocation[1] == "":  # If no location is specified by user
    print "No location specified"         #then see if location exists in first word of first line of article and use that
    articles = api.search(q = wordSearch, begin_date = searchDateStart, end_date = searchDateEnd, fl = ['web_url', 'lead_paragraph', 'abstract', 'source', 'headline'])
    accessKey = articles['response']['docs']  # Create variable that accesses keys of nested dictionaries from Article Search results

    for ele in accessKey:  # Loop through each element in the Article Search nested dictionaries
        webURL = ele['web_url']  # Access web URL key in nested dictionary and assign to variable
        headline = ele['headline']['main']  # Access main headline in nested dictionary and assign to variable.
        HLURLnoLoc[headline] = webURL  # Updates empty dictionary with headline:URL pairs
        print webURL  # Test
    #print len(articles)

else:  # If user specifies location
    geoLocDict = {geoLocation[0]:geoLocation[1]}
    print geoLocDict
    articles = api.search(q = wordSearch, fq = geoLocDict, begin_date = searchDateStart, end_date = searchDateEnd, fl = ['web_url', 'lead_paragraph', 'abstract', 'source', 'headline'])
    accessKey = articles['response']['docs']  # Create variable that accesses keys of nested dictionaries from Article Search results
    #print len(articles)

#make sure blank gui inputs are .........dealt with    if line = "", don't add it to list or something
#figure out how to connect the article search api with the geographic api to get the coordinates associated with the queried articles so that we can map them
#also figure out how to make a dictionary from the field query types such that user inputs for the field query lines in text file are added as values for the correct
#field query type and if there are multiple inputs for one line (ex. word, word, word) make that value a list of those words fq = {headline:Obama, body:[whale, snake]}



#check to see if i can have multiple inputs for glocations: or if i need to use
#glocations.contains for when there is multiple inputs (can just say that only one location
#input allowed to make things easy and allow the map to zoom to only one area)

#instead of saving user inputs, coud use the function entry = E1.get()



#if user leave blank, make sure that is okay
#1 key, multiple values for glocations.contains, make sure that works, have to make a list for values

#if/while/for functions so that each time the go button is clicked, saves a new text file,
#have the number at end of text file name increase by 1 or something (not sure how to do)

#geoparcer API service - used GeoNames

#or 

#if user input location into gui, use to zoom for map and query articles

#if no user location input (""), see if there is a location name as the first
#word in the first line of the article, if there isn't, location will be New York City and use that to zoom for map
#NYC used for geocoding.

#if no user location input (""), if there is a location name as the first word in
#the first line of the article, run it through the list of domestic city, state, state abriviation to
#see if match for each article, and if there is, map based on this, if not, do NYC again"""


############################################################################################## Amy's Script
url = 'https://geoparser.io/api/geoparser' # URL for Geoparser.io service
headers = {'Authorization': 'apiKey 96371282185817752'} # API key for Geoparser

HLURLVals = HLURLnoLoc.values()  # Access dictionary URL values as a list and assign to variable

for val in HLURLVals:  # For loop to extract URLs
    r = requests.get(val)  # Create response object to make requests from Geoparser
    HTML = BeautifulSoup(r.text)  # Create object to parse HTML text from webpage using Beautiful Soup and HTMLParser standard library
    articleTxt = [''.join(s.findAll(text=True)) for s in HTML.findAll('p')]  # Create list, find all HTML text elements, create for loop to find all paragraph elements in HTML, then join text to empty string. This converts all HTML text to string within list

    lstVal = [val]  # Create list of URL values
    for ch in lstVal: # For loop to iterate through each letter in list of URL characters
        if ch[23:28] == 'video':  # If characters within specified URL index values = 'video', return index 0
            artSlice = articleTxt[0]  # Slice list of article text to get first line of video description
        else:  # For all other scenarios, index for first 9 lines of the article
            artSlice = articleTxt[1:10]
    GPinput = ''  # Initialize empty string

    for item in artSlice:  # For loop to extract each line of body text and join to empty string
        GPinput = GPinput + ''.join(item)  # Join each line of body text to empty string
        data = {'inputText': GPinput}  # Passing HTML text into Geoparser API
        resp = requests.post(url, headers=headers, data=data)  # Create another response object to make request for Geoparser output
        parsed_json = json.loads(resp.text)  # Decode json output from Geoparser

    if parsed_json['features'] == []:
        coordinateList = [[-73.9904, 40.7562], "New York Times building", "NY", "US"]
        print coordinateList
    else:
        placeCoordinates = parsed_json['features'][0]['geometry']['coordinates']  # Index for coordinates of location in article
        placeName = parsed_json['features'][0]['properties']['name']  # Index for name of location in article
        placeADM1 = parsed_json['features'][0]['properties']['admin1']  # Index for first-level administrative district of location in article
        placeCountry = parsed_json['features'][0]['properties']['country']  # Index for country of location in article
        LocFound = [placeCoordinates, placeName, placeADM1, placeCountry]
        print LocFound

print 'success'


"""


############################################################################################# Haoyu's Script
inputWorkspace = r'H:\GISComputerProgramming\FinalProject'
api = articleAPI('8a7bed87ddf84a98bd560ba2eb3ba3fb') # API key

articles = api.search(q = wordSearch, fq = {geoLocDict}, begin_date = searchDateStart, end_date = searchDateEnd, fl = ['web_url', 'lead_paragraph', 'abstract', 'source', 'headline', 'pub_date', 'byline'])


geolocator = Nominatim()
location = geolocator.geocode(geoLocDict)


#print len(articles['response']['docs'])#f = keywords? indexing mess up?

i = 0
titles = []
# loop every news record
while i < len(articles['response']['docs']):
    # append head line 
    titles.append(articles['response']['docs'][i]['headline']['main'])# headLine
    i= i+1

u = 0
urls = []
# loop every news records
while u < len(articles['response']['docs']):
    # append url to list
    urls.append(articles['response']['docs'][u]['web_url']) # urls
    u= u+1


geolocator = Nominatim()
location = geolocator.geocode('New York')

print location.raw['lon'], location.raw['lat']#location.raw['boundingbox']

lon = location.raw['lon']
lat = location.raw['lat']
#Dynamic

header = ["Lat", "Lon", "Title", "URL"]
#rows = [lat, lon, title, url]
csvName = "test.csv"
testCsv = os.path.join(inputWorkspace, csvName)
f = open(testCsv, 'wb')
w = csv.writer(f, delimiter = ',')
w.writerow(header)

#print title[1]


t = 0
while t < len(articles['response']['docs']):
    print t
    # title = str(titles[t])
    title = titles[t].encode('utf-8')
    title_origin = titles[t]
    print title
    print title_origin
    # url = str(urls[t])
    url = urls[t].encode('utf-8')
    url_origin = urls[t]
    print url
    print url_origin
    rows = [lat, lon, title, url]
    w.writerow(rows)
    t = t+ 1    

f.close()


arcpy.env.workspace = r'H:\GISComputerProgramming\FinalProject'
print 1
output = 'Geocode'
arcpy.MakeXYEventLayer_management(testCsv, 'Lat', 'Lon', output)
print output
saved = 'result.lyr'
arcpy.SaveToLayerFile_management(output, saved)
print saved
"""
